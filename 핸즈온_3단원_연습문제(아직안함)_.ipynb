{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "í•¸ì¦ˆì˜¨ 3ë‹¨ì› ì—°ìŠµë¬¸ì œ(ì•„ì§ì•ˆí•¨) .ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOB7OiOzT3t4a6+RfzgBtir",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dabin-Park/Handson/blob/main/%ED%95%B8%EC%A6%88%EC%98%A8_3%EB%8B%A8%EC%9B%90_%EC%97%B0%EC%8A%B5%EB%AC%B8%EC%A0%9C(%EC%95%84%EC%A7%81%EC%95%88%ED%95%A8)_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqxxZJHBOX-Z"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "kQ3Hyi-jjnzK"
      },
      "source": [
        "# ì—°ìŠµë¬¸ì œ í•´ë‹µ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yetIFx-qjnzK"
      },
      "source": [
        "## 1. 97% ì •í™•ë„ì˜ MNIST ë¶„ë¥˜ê¸°"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25jqiTsBjnzL"
      },
      "source": [
        "**ê²½ê³ **: ì‚¬ìš©í•˜ëŠ” í•˜ë“œì›¨ì–´ì— ë”°ë¼ ë‹¤ìŒ ì…€ì„ ì‹¤í–‰í•˜ëŠ”ë° 16ì‹œê°„ ë˜ëŠ” ê·¸ ì´ìƒ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSHCy7yljnzL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e632c86-c165-494d-ed21-9f7c686a895c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = [{'weights': [\"uniform\", \"distance\"], 'n_neighbors': [3, 4, 5]}]\n",
        "\n",
        "knn_clf = KNeighborsClassifier()\n",
        "grid_search = GridSearchCV(knn_clf, param_grid, cv=5, verbose=3)\n",
        "grid_search.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGNRIGbMjnzL"
      },
      "outputs": [],
      "source": [
        "grid_search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYJS27qbjnzL"
      },
      "outputs": [],
      "source": [
        "grid_search.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRqZZYwIjnzL"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = grid_search.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TPfS5WjjnzL"
      },
      "source": [
        "## 2. ë°ì´í„° ì¦ì‹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-qQny0EjnzL"
      },
      "outputs": [],
      "source": [
        "from scipy.ndimage.interpolation import shift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAIlWn7CjnzL"
      },
      "outputs": [],
      "source": [
        "def shift_image(image, dx, dy):\n",
        "    image = image.reshape((28, 28))\n",
        "    shifted_image = shift(image, [dy, dx], cval=0, mode=\"constant\")\n",
        "    return shifted_image.reshape([-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4KB6hayjnzM"
      },
      "outputs": [],
      "source": [
        "image = X_train[1000]\n",
        "shifted_image_down = shift_image(image, 0, 5)\n",
        "shifted_image_left = shift_image(image, -5, 0)\n",
        "\n",
        "plt.figure(figsize=(12,3))\n",
        "plt.subplot(131)\n",
        "plt.title(\"Original\", fontsize=14)\n",
        "plt.imshow(image.reshape(28, 28), interpolation=\"nearest\", cmap=\"Greys\")\n",
        "plt.subplot(132)\n",
        "plt.title(\"Shifted down\", fontsize=14)\n",
        "plt.imshow(shifted_image_down.reshape(28, 28), interpolation=\"nearest\", cmap=\"Greys\")\n",
        "plt.subplot(133)\n",
        "plt.title(\"Shifted left\", fontsize=14)\n",
        "plt.imshow(shifted_image_left.reshape(28, 28), interpolation=\"nearest\", cmap=\"Greys\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03k5bPS-jnzM"
      },
      "outputs": [],
      "source": [
        "X_train_augmented = [image for image in X_train]\n",
        "y_train_augmented = [label for label in y_train]\n",
        "\n",
        "for dx, dy in ((1, 0), (-1, 0), (0, 1), (0, -1)):\n",
        "    for image, label in zip(X_train, y_train):\n",
        "        X_train_augmented.append(shift_image(image, dx, dy))\n",
        "        y_train_augmented.append(label)\n",
        "\n",
        "X_train_augmented = np.array(X_train_augmented)\n",
        "y_train_augmented = np.array(y_train_augmented)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CM_C88j7jnzM"
      },
      "outputs": [],
      "source": [
        "shuffle_idx = np.random.permutation(len(X_train_augmented))\n",
        "X_train_augmented = X_train_augmented[shuffle_idx]\n",
        "y_train_augmented = y_train_augmented[shuffle_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "536dpO4mjnzM"
      },
      "outputs": [],
      "source": [
        "knn_clf = KNeighborsClassifier(**grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTNqkkT9jnzM"
      },
      "outputs": [],
      "source": [
        "knn_clf.fit(X_train_augmented, y_train_augmented)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BfmRxgJjnzN"
      },
      "source": [
        "**ê²½ê³ **: ì‚¬ìš©í•˜ëŠ” í•˜ë“œì›¨ì–´ì— ë”°ë¼ ë‹¤ìŒ ì…€ì„ ì‹¤í–‰í•˜ëŠ”ë° 1ì‹œê°„ ë˜ëŠ” ê·¸ ì´ìƒ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4308nszjnzN"
      },
      "outputs": [],
      "source": [
        "y_pred = knn_clf.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocrjFLdtjnzN"
      },
      "source": [
        "ê°„ë‹¨íˆ ë°ì´í„°ë¥¼ ì¦ì‹í•´ì„œ 0.5% ì •í™•ë„ë¥¼ ë†’ì˜€ìŠµë‹ˆë‹¤. :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2Qy6mmjjnzN"
      },
      "source": [
        "## 3. íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ ë„ì „"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWLvnS80jnzN"
      },
      "source": [
        "ìŠ¹ê°ì˜ ë‚˜ì´, ì„±ë³„, ìŠ¹ê° ë“±ê¸‰, ìŠ¹ì„  ìœ„ì¹˜ ê°™ì€ ì†ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ ìŠ¹ê°ì˜ ìƒì¡´ ì—¬ë¶€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xItT5LGMjnzN"
      },
      "source": [
        "ë¨¼ì € ë°ì´í„°ë¥¼ ë¡œë“œí•´ ë³´ì£ :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZpz80E-jnzN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "TITANIC_PATH = os.path.join(\"datasets\", \"titanic\")\n",
        "DOWNLOAD_URL = \"https://raw.githubusercontent.com/rickiepark/handson-ml2/master/datasets/titanic/\"\n",
        "\n",
        "def fetch_titanic_data(url=DOWNLOAD_URL, path=TITANIC_PATH):\n",
        "    if not os.path.isdir(path):\n",
        "        os.makedirs(path)\n",
        "    for filename in (\"train.csv\", \"test.csv\"):\n",
        "        filepath = os.path.join(path, filename)\n",
        "        if not os.path.isfile(filepath):\n",
        "            print(\"Downloading\", filename)\n",
        "            urllib.request.urlretrieve(url + filename, filepath)\n",
        "\n",
        "fetch_titanic_data()   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jf-ne7e5jnzN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_titanic_data(filename, titanic_path=TITANIC_PATH):\n",
        "    csv_path = os.path.join(titanic_path, filename)\n",
        "    return pd.read_csv(csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m9Fbi3EjnzN"
      },
      "outputs": [],
      "source": [
        "train_data = load_titanic_data(\"train.csv\")\n",
        "test_data = load_titanic_data(\"test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CLsFef8jnzN"
      },
      "source": [
        "ë°ì´í„°ëŠ” ì´ë¯¸ í›ˆë ¨ ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„ë¦¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ë ˆì´ë¸”ì„ ê°€ì§€ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤: í›ˆë ¨ ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ ê°€ëŠ¥í•œ ìµœê³ ì˜ ëª¨ë¸ì„ ë§Œë“¤ê³  í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìºê¸€(Kaggle)ì— ì—…ë¡œë“œí•˜ì—¬ ìµœì¢… ì ìˆ˜ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhKaVDGJjnzN"
      },
      "source": [
        "í›ˆë ¨ ì„¸íŠ¸ì—ì„œ ë§¨ ìœ„ ëª‡ ê°œì˜ ì—´ì„ ì‚´í´ ë³´ê² ìŠµë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhZN9kQijnzN"
      },
      "outputs": [],
      "source": [
        "train_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU6cWtt2jnzN"
      },
      "source": [
        "ì†ì„±ì€ ë‹¤ìŒê³¼ ê°™ì€ ì˜ë¯¸ë¥¼ ê°€ì§‘ë‹ˆë‹¤:\n",
        "* **PassengerId**: ê° ìŠ¹ê°ì˜ ê³ ìœ  ì‹ë³„ì.\n",
        "* **Survived**: íƒ€ê¹ƒì…ë‹ˆë‹¤. 0ì€ ìƒì¡´í•˜ì§€ ëª»í•œ ê²ƒì´ê³  1ì€ ìƒì¡´ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
        "* **Pclass**: ìŠ¹ê° ë“±ê¸‰. 1, 2, 3ë“±ì„.\n",
        "* **Name**, **Sex**, **Age**: ì´ë¦„ ê·¸ëŒ€ë¡œ ì˜ë¯¸ì…ë‹ˆë‹¤.\n",
        "* **SibSp**: í•¨ê»˜ íƒ‘ìŠ¹í•œ í˜•ì œ, ë°°ìš°ìì˜ ìˆ˜.\n",
        "* **Parch**: í•¨ê»˜ íƒ‘ìŠ¹í•œ ìë…€, ë¶€ëª¨ì˜ ìˆ˜.\n",
        "* **Ticket**: í‹°ì¼“ ì•„ì´ë””\n",
        "* **Fare**: í‹°ì¼“ ìš”ê¸ˆ (íŒŒìš´ë“œ)\n",
        "* **Cabin**: ê°ì‹¤ ë²ˆí˜¸\n",
        "* **Embarked**: ìŠ¹ê°ì´ íƒ‘ìŠ¹í•œ ê³³. C(Cherbourg), Q(Queenstown), S(Southampton)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNhFX_qyjnzN"
      },
      "source": [
        "`PassengerId` ì—´ì„ ì¸ë±ìŠ¤ ì—´ë¡œ ì§€ì •í•˜ê² ìŠµë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3P2cyXWHjnzN"
      },
      "outputs": [],
      "source": [
        "train_data = train_data.set_index(\"PassengerId\")\n",
        "test_data = test_data.set_index(\"PassengerId\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGt2t3QRjnzN"
      },
      "source": [
        "ëˆ„ë½ëœ ë°ì´í„°ê°€ ì–¼ë§ˆë‚˜ ë˜ëŠ”ì§€ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSr323XKjnzN"
      },
      "outputs": [],
      "source": [
        "train_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUztY200jnzN"
      },
      "outputs": [],
      "source": [
        "train_data[train_data[\"Sex\"]==\"female\"][\"Age\"].median()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x330gkKsjnzN"
      },
      "source": [
        "ì¢‹ìŠµë‹ˆë‹¤. **Age**, **Cabin**, **Embarked** ì†ì„±ì˜ ì¼ë¶€ê°€ nullì…ë‹ˆë‹¤(891ê°œì˜ non-null ë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤). íŠ¹íˆ **Cabin**ì€ 77%ê°€ nullì…ë‹ˆë‹¤. ì¼ë‹¨ **Cabin**ì€ ë¬´ì‹œí•˜ê³  ë‚˜ë¨¸ì§€ë¥¼ í™œìš©í•˜ê² ìŠµë‹ˆë‹¤. **Age**ëŠ” 19%ê°€ nullì´ë¯€ë¡œ ì´ë¥¼ ì–´ë–»ê²Œ ì²˜ë¦¬í• ì§€ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤. nullì„ ì¤‘ê°„ ë‚˜ì´ë¡œ ë°”ê¾¸ëŠ” ê²ƒì´ ê´œì°®ì•„ ë³´ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ì—´ì„ ì‚¬ìš©í•˜ì—¬ ë‚˜ì´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ(ì˜ˆë¥¼ ë“¤ì–´, 1ë“±ì„ì˜ ì¤‘ê°„ ë‚˜ì´ëŠ” 37, 2ë“±ì„ì€ 29, 3ë“±ì„ì€ 24ì…ë‹ˆë‹¤)ì´ ì¡°ê¸ˆ í˜„ëª…í•´ ë³´ì¼ ìˆ˜ ìˆì§€ë§Œ ë‹¨ìˆœí•¨ì„ ìœ„í•´ ì „ì²´ì˜ ì¤‘ê°„ ë‚˜ì´ë¥¼ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1bgsZNHjnzN"
      },
      "source": [
        "**Name**ê³¼ **Ticket** ì†ì„±ë„ ê°’ì„ ê°€ì§€ê³  ìˆì§€ë§Œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì´ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì´ ì¡°ê¸ˆ ê¹Œë‹¤ë¡­ìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ì§€ê¸ˆì€ ì´ ë‘ ì†ì„±ì„ ë¬´ì‹œí•˜ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hkbOc6rjnzO"
      },
      "source": [
        "í†µê³„ì¹˜ë¥¼ ì‚´í´ ë³´ê² ìŠµë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xo52uxz_jnzO"
      },
      "outputs": [],
      "source": [
        "train_data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9xM1FOyjnzO"
      },
      "source": [
        "* ì´í¬, 38%ë§Œ **Survived**ì…ë‹ˆë‹¤. ğŸ˜­ ê±°ì˜ 40%ì— ê°€ê¹Œìš°ë¯€ë¡œ ì •í™•ë„ë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ì„ í‰ê°€í•´ë„ ê´œì°®ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.\n",
        "* í‰ê·  **Fare**ëŠ” 32.20 íŒŒìš´ë“œë¼ ê·¸ë ‡ê²Œ ë¹„ì‹¸ë³´ì´ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤(ì•„ë§ˆ ìš”ê¸ˆì„ ë§ì´ ë°˜í™˜í•´ ì£¼ì—ˆê¸° ë•Œë¬¸ì¼ ê²ƒì…ë‹ˆë‹¤)\n",
        "* í‰ê·  **Age**ëŠ” 30ë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm__Fqs2jnzO"
      },
      "source": [
        "íƒ€ê¹ƒì´ 0ê³¼ 1ë¡œ ì´ë£¨ì–´ì¡ŒëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhGVk5lzjnzO"
      },
      "outputs": [],
      "source": [
        "train_data[\"Survived\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY7EjwSFjnzO"
      },
      "source": [
        "ë²”ì£¼í˜• íŠ¹ì„±ë“¤ì„ í™•ì¸í•´ ë³´ê² ìŠµë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RuMnimFjnzO"
      },
      "outputs": [],
      "source": [
        "train_data[\"Pclass\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdCVLAT9jnzO"
      },
      "outputs": [],
      "source": [
        "train_data[\"Sex\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl5ieoRljnzO"
      },
      "outputs": [],
      "source": [
        "train_data[\"Embarked\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tFT4_cljnzO"
      },
      "source": [
        "**Embarked** íŠ¹ì„±ì€ ìŠ¹ê°ì´ íƒ‘ìŠ¹í•œ ê³³ì„ ì•Œë ¤ ì¤ë‹ˆë‹¤: C=Cherbourg, Q=Queenstown, S=Southampton."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sl3LNUZwjnzO"
      },
      "source": [
        "ìˆ˜ì¹˜ íŠ¹ì„±ì„ ìœ„í•œ íŒŒì´í”„ë¼ì¸ë¶€í„° ì‹œì‘í•´ì„œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ë§Œë“¤ì–´ ë³´ì£ :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AytaHpcijnzO"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler())\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKlZFX23jnzO"
      },
      "source": [
        "ì´ì œ ë²”ì£¼í˜• íŠ¹ì„±ì„ ìœ„í•œ íŒŒì´í”„ë¼ì¸ì„ ë§Œë“­ë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDNSmmpPjnzO"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwR1_XcajnzO"
      },
      "outputs": [],
      "source": [
        "cat_pipeline = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"cat_encoder\", OneHotEncoder(sparse=False)),\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNRHSnN7jnzO"
      },
      "source": [
        "ë§ˆì§€ë§‰ìœ¼ë¡œ ìˆ«ìì™€ ë²”ì£¼í˜• íŒŒì´í”„ë¼ì¸ì„ ì—°ê²°í•©ë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8S34eq-2jnzO"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "num_attribs = [\"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n",
        "cat_attribs = [\"Pclass\", \"Sex\", \"Embarked\"]\n",
        "\n",
        "preprocess_pipeline = ColumnTransformer([\n",
        "        (\"num\", num_pipeline, num_attribs),\n",
        "        (\"cat\", cat_pipeline, cat_attribs),\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5h_jg0MjnzO"
      },
      "source": [
        "ì¢‹ìŠµë‹ˆë‹¤! ì´ì œ ì›ë³¸ ë°ì´í„°ë¥¼ ë°›ì•„ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì— ì£¼ì…í•  ìˆ«ì ì…ë ¥ íŠ¹ì„±ì„ ì¶œë ¥í•˜ëŠ” ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGRqUav3jnzO"
      },
      "outputs": [],
      "source": [
        "X_train = preprocess_pipeline.fit_transform(\n",
        "    train_data[num_attribs + cat_attribs])\n",
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBtZOu0MjnzP"
      },
      "source": [
        "ë ˆì´ë¸”ì„ ê°€ì ¸ì˜µë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVtzuAbPjnzP"
      },
      "outputs": [],
      "source": [
        "y_train = train_data[\"Survived\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XifgoH33jnzP"
      },
      "source": [
        "ì´ì œ ë¶„ë¥˜ê¸°ë¥¼ í›ˆë ¨ì‹œí‚¬ ì°¨ë¡€ì…ë‹ˆë‹¤. ë¨¼ì € `RandomForestClassifier`ë¥¼ ì‚¬ìš©í•´ ë³´ê² ìŠµë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGOS6fxujnzP"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "forest_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR_reOIFjnzP"
      },
      "source": [
        "ëª¨ë¸ì´ ì˜ í›ˆë ¨ëœ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì´ë¥¼ ì‚¬ìš©í•´ì„œ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ë§Œë“­ë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKAsbjG0jnzP"
      },
      "outputs": [],
      "source": [
        "X_test = preprocess_pipeline.transform(test_data[num_attribs + cat_attribs])\n",
        "y_pred = forest_clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz2eVcUfjnzP"
      },
      "source": [
        "ì´ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ (ìºê¸€ì—ì„œ ê¸°ëŒ€í•˜ëŠ” í˜•íƒœì¸) CSV íŒŒì¼ë¡œ ë§Œë“¤ì–´ ì—…ë¡œë“œí•˜ê³  í‰ê°€ë¥¼ ë°›ì•„ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ê·¸ëƒ¥ ì¢‹ì„ê±°ë¼ ê¸°ëŒ€í•˜ëŠ” ê²ƒë³´ë‹¤ êµì°¨ ê²€ì¦ìœ¼ë¡œ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€ í‰ê°€í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY9mD4PujnzP"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "forest_scores = cross_val_score(forest_clf, X_train, y_train, cv=10)\n",
        "forest_scores.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1m-UetvjnzP"
      },
      "source": [
        "ì•„ì£¼ ë‚˜ì˜ì§€ ì•Šë„¤ìš”! ìºê¸€ì—ì„œ íƒ€ì´íƒ€ë‹‰ ê²½ì—° ëŒ€íšŒì˜ [ë¦¬ë”ë³´ë“œ](https://www.kaggle.com/c/titanic/leaderboard)ì—ì„œ ìƒìœ„ 2% ì•ˆì— ë“  ì ìˆ˜ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì™€ìš°! ì–´ë–¤ ì‚¬ëŒë“¤ì€ 100%ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ íƒ€ì´íƒ€ë‹‰ì˜ [í¬ìƒì ëª©ë¡](https://www.encyclopedia-titanica.org/titanic-victims/)ì„ ì‰½ê²Œ ì°¾ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¨¸ì‹ ëŸ¬ë‹ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³ ë„ ì´ëŸ° ì •í™•ë„ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸ˜†"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNuD4LamjnzP"
      },
      "source": [
        "`SVC`ë¥¼ ì ìš©í•´ ë³´ê² ìŠµë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejhp8xHwjnzP"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_clf = SVC(gamma=\"auto\")\n",
        "svm_scores = cross_val_score(svm_clf, X_train, y_train, cv=10)\n",
        "svm_scores.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A8T_cbQjnzP"
      },
      "source": [
        "ì¢‹ë„¤ìš”! ì´ ëª¨ë¸ì´ í›¨ì”¬ ë‚˜ì•„ë³´ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpNHdlIljnzP"
      },
      "source": [
        "í•˜ì§€ë§Œ 10 í´ë“œ êµì°¨ ê²€ì¦ì— ëŒ€í•œ í‰ê·  ì •í™•ë„ë¥¼ ë³´ëŠ” ëŒ€ì‹  ëª¨ë¸ì—ì„œ ì–»ì€ 10ê°œì˜ ì ìˆ˜ë¥¼ 1ì‚¬ë¶„ìœ„, 3ì‚¬ë¶„ìœ„ë¥¼ ëª…ë£Œí•˜ê²Œ í‘œí˜„í•´ì£¼ëŠ” ìƒì ìˆ˜ì—¼ ê·¸ë¦¼(box-and-whisker) ê·¸ë˜í”„ë¥¼ ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤(ì´ ë°©ì‹ì„ ì œì•ˆí•´ ì¤€ Nevin Yilmazì—ê²Œ ê°ì‚¬í•©ë‹ˆë‹¤). `boxplot()` í•¨ìˆ˜ëŠ” ì´ìƒì¹˜(í”Œë¼ì´ì–´(flier)ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤)ë¥¼ ê°ì§€í•˜ê³  ìˆ˜ì—¼ ë¶€ë¶„ì— ì´ë¥¼ í¬í•¨ì‹œí‚¤ì§€ ì•ŠìŠµë‹ˆë‹¤. 1ì‚¬ë¶„ìœ„ê°€ $Q_1$ì´ê³  3ì‚¬ë¶„ìœ„ê°€ $Q_3$ì´ë¼ë©´ ì‚¬ë¶„ìœ„ìˆ˜ ë²”ìœ„ëŠ” $IQR = Q_3 - Q_1$ê°€ ë©ë‹ˆë‹¤(ì´ ê°’ì´ ë°•ìŠ¤ì˜ ë†’ì´ê°€ ë©ë‹ˆë‹¤). $Q_1 - 1.5 \\times IQR$ ë³´ë‹¤ ë‚®ê±°ë‚˜ $Q3 + 1.5 \\times IQR$ ë³´ë‹¤ ë†’ì€ ì ìˆ˜ëŠ” ì´ìƒì¹˜ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeK3goRLjnzP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot([1]*10, svm_scores, \".\")\n",
        "plt.plot([2]*10, forest_scores, \".\")\n",
        "plt.boxplot([svm_scores, forest_scores], labels=(\"SVM\",\"Random Forest\"))\n",
        "plt.ylabel(\"Accuracy\", fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaeQz5a0jnzP"
      },
      "source": [
        "ëœë¤ í¬ë ˆìŠ¤íŠ¸ ë¶„ë¥˜ê¸°ê°€ 10ê°œì˜ í´ë“œ ì¤‘ í•˜ë‚˜ì—ì„œ ë§¤ìš° ë†’ì€ ì ìˆ˜ë¥¼ ì–»ì—ˆì§€ë§Œ ë„“ê²Œ í¼ì ¸ ìˆê¸° ë•Œë¬¸ì— ì „ì²´ì ì¸ í‰ê·  ì ìˆ˜ëŠ” ë‚®ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ SVM ë¶„ë¥˜ê¸°ê°€ ì¼ë°˜í™”ë¥¼ ë” ì˜í•˜ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-k4mjC_jnzP"
      },
      "source": [
        "ì´ ê²°ê³¼ë¥¼ ë” í–¥ìƒì‹œí‚¤ë ¤ë©´:\n",
        "* êµì°¨ ê²€ì¦ê³¼ ê·¸ë¦¬ë“œ íƒìƒ‰ì„ ì‚¬ìš©í•˜ì—¬ ë” ë§ì€ ëª¨ë¸ì„ ë¹„êµí•˜ê³  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•˜ì„¸ìš”.\n",
        "* íŠ¹ì„± ê³µí•™ì„ ë” ì‹œë„í•´ ë³´ì„¸ìš”, ì˜ˆë¥¼ ë“¤ë©´:\n",
        "  * ìˆ˜ì¹˜ íŠ¹ì„±ì„ ë²”ì£¼í˜• íŠ¹ì„±ìœ¼ë¡œ ë°”ê¾¸ì–´ ë³´ì„¸ìš”: ì˜ˆë¥¼ ë“¤ì–´, ë‚˜ì´ëŒ€ê°€ ë‹¤ë¥¸ ê²½ìš° ë‹¤ë¥¸ ìƒì¡´ ë¹„ìœ¨ì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤(ì•„ë˜ ì°¸ì¡°). ê·¸ëŸ¬ë¯€ë¡œ ë‚˜ì´ êµ¬ê°„ì„ ë²”ì£¼ë¡œ ë§Œë“¤ì–´ ë‚˜ì´ ëŒ€ì‹  ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë„ì›€ì´ ë  ìˆ˜ ìˆìŠ¤ë‹ˆë‹¤. ë¹„ìŠ·í•˜ê²Œ ìƒì¡´ìì˜ 30%ê°€ í˜¼ì ì—¬í–‰í•˜ëŠ” ì‚¬ëŒì´ê¸° ë•Œë¬¸ì— ì´ë“¤ì„ ìœ„í•œ íŠ¹ë³„í•œ ë²”ì£¼ë¥¼ ë§Œë“œëŠ” ê²ƒì´ ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ì•„ë˜ ì°¸ì¡°).\n",
        "  * **SibSp**ì™€ **Parch**ì„ ì´ ë‘ íŠ¹ì„±ì˜ í•©ìœ¼ë¡œ ë°”ê¿‰ë‹ˆë‹¤.\n",
        "  * **Survived** íŠ¹ì„±ê³¼ ê´€ë ¨ëœ ì´ë¦„ì„ êµ¬ë³„í•´ ë³´ì„¸ìš”.\n",
        "  * **Cabin** ì—´ì„ ì‚¬ìš©í•˜ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´ ì²« ê¸€ìë¥¼ ë²”ì£¼í˜• ì†ì„±ì²˜ëŸ¼ ë‹¤ë£° ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQvVUii0jnzP"
      },
      "outputs": [],
      "source": [
        "train_data[\"AgeBucket\"] = train_data[\"Age\"] // 15 * 15\n",
        "train_data[[\"AgeBucket\", \"Survived\"]].groupby(['AgeBucket']).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibI5Oyw-jnzP"
      },
      "outputs": [],
      "source": [
        "train_data[\"RelativesOnboard\"] = train_data[\"SibSp\"] + train_data[\"Parch\"]\n",
        "train_data[[\"RelativesOnboard\", \"Survived\"]].groupby(['RelativesOnboard']).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjt6r2fqjnzP"
      },
      "source": [
        "## 4. ìŠ¤íŒ¸ í•„í„°"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTQHmzOVjnzQ"
      },
      "source": [
        "ë¨¼ì € ë°ì´í„°ë¥¼ ë‹¤ìš´ë°›ìŠµë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emXJknqTjnzQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "import urllib.request\n",
        "\n",
        "DOWNLOAD_ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
        "HAM_URL = DOWNLOAD_ROOT + \"20030228_easy_ham.tar.bz2\"\n",
        "SPAM_URL = DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\"\n",
        "SPAM_PATH = os.path.join(\"datasets\", \"spam\")\n",
        "\n",
        "def fetch_spam_data(ham_url=HAM_URL, spam_url=SPAM_URL, spam_path=SPAM_PATH):\n",
        "    if not os.path.isdir(spam_path):\n",
        "        os.makedirs(spam_path)\n",
        "    for filename, url in ((\"ham.tar.bz2\", ham_url), (\"spam.tar.bz2\", spam_url)):\n",
        "        path = os.path.join(spam_path, filename)\n",
        "        if not os.path.isfile(path):\n",
        "            urllib.request.urlretrieve(url, path)\n",
        "        tar_bz2_file = tarfile.open(path)\n",
        "        tar_bz2_file.extractall(path=spam_path)\n",
        "        tar_bz2_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqkvnKqejnzQ"
      },
      "outputs": [],
      "source": [
        "fetch_spam_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWG1JBjLjnzQ"
      },
      "source": [
        "ë‹¤ìŒ, ëª¨ë“  ì´ë©”ì¼ì„ ì½ì–´ ë“¤ì…ë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8QrIzCPjnzQ"
      },
      "outputs": [],
      "source": [
        "HAM_DIR = os.path.join(SPAM_PATH, \"easy_ham\")\n",
        "SPAM_DIR = os.path.join(SPAM_PATH, \"spam\")\n",
        "ham_filenames = [name for name in sorted(os.listdir(HAM_DIR)) if len(name) > 20]\n",
        "spam_filenames = [name for name in sorted(os.listdir(SPAM_DIR)) if len(name) > 20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YU-QD98ZjnzQ"
      },
      "outputs": [],
      "source": [
        "len(ham_filenames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scudlSF8jnzQ"
      },
      "outputs": [],
      "source": [
        "len(spam_filenames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0HYMSKrjnzQ"
      },
      "source": [
        "íŒŒì´ì¬ì˜ `email` ëª¨ë“ˆì„ ì‚¬ìš©í•´ ì´ë©”ì¼ì„ íŒŒì‹±í•©ë‹ˆë‹¤(í—¤ë”, ì¸ì½”ë”© ë“±ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-x0TwsoIjnzQ"
      },
      "outputs": [],
      "source": [
        "import email\n",
        "import email.policy\n",
        "\n",
        "def load_email(is_spam, filename, spam_path=SPAM_PATH):\n",
        "    directory = \"spam\" if is_spam else \"easy_ham\"\n",
        "    with open(os.path.join(spam_path, directory, filename), \"rb\") as f:\n",
        "        return email.parser.BytesParser(policy=email.policy.default).parse(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkLEM7F4jnzQ"
      },
      "outputs": [],
      "source": [
        "ham_emails = [load_email(is_spam=False, filename=name) for name in ham_filenames]\n",
        "spam_emails = [load_email(is_spam=True, filename=name) for name in spam_filenames]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLDHmVAAjnzQ"
      },
      "source": [
        "ë°ì´í„°ê°€ ì–´ë–»ê²Œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ì§€ ê°ì„ ì¡ê¸° ìœ„í•´ í–„ ë©”ì¼ê³¼ ìŠ¤íŒ¸ ë©”ì¼ì„ í•˜ë‚˜ì”© ë³´ê² ìŠµë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCn1UyphjnzQ"
      },
      "outputs": [],
      "source": [
        "print(ham_emails[1].get_content().strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFF-gyHijnzQ"
      },
      "outputs": [],
      "source": [
        "print(spam_emails[6].get_content().strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLet52w3jnzQ"
      },
      "source": [
        "ì–´ë–¤ ì´ë©”ì¼ì€ ì´ë¯¸ì§€ë‚˜ ì²¨ë¶€ íŒŒì¼ì„ ê°€ì§„ ë©€í‹°íŒŒíŠ¸(multipart)ì…ë‹ˆë‹¤(ë©”ì¼ì— í¬í•¨ë˜ì–´ ìˆì„ìˆ˜ ìˆìŠµë‹ˆë‹¤). ì–´ë–¤ íŒŒì¼ë“¤ì´ ìˆëŠ”ì§€ ì‚´í´ ë³´ê² ìŠµë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnE6VK8ajnzQ"
      },
      "outputs": [],
      "source": [
        "def get_email_structure(email):\n",
        "    if isinstance(email, str):\n",
        "        return email\n",
        "    payload = email.get_payload()\n",
        "    if isinstance(payload, list):\n",
        "        return \"multipart({})\".format(\", \".join([\n",
        "            get_email_structure(sub_email)\n",
        "            for sub_email in payload\n",
        "        ]))\n",
        "    else:\n",
        "        return email.get_content_type()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOFW4_S0jnzQ"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def structures_counter(emails):\n",
        "    structures = Counter()\n",
        "    for email in emails:\n",
        "        structure = get_email_structure(email)\n",
        "        structures[structure] += 1\n",
        "    return structures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhPHYzsKjnzQ"
      },
      "outputs": [],
      "source": [
        "structures_counter(ham_emails).most_common()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orIvamhGjnzQ"
      },
      "outputs": [],
      "source": [
        "structures_counter(spam_emails).most_common()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3u9vYQRjnzQ"
      },
      "source": [
        "í–„ ë©”ì¼ì€ í‰ë²”í•œ í…ìŠ¤íŠ¸ê°€ ë§ê³  ìŠ¤íŒ¸ì€ HTMLì¼ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ì ì€ ìˆ˜ì˜ í–„ ì´ë©”ì¼ì´ PGPë¡œ ì„œëª…ë˜ì–´ ìˆì§€ë§Œ ìŠ¤íŒ¸ ë©”ì¼ì—ëŠ” ì—†ìŠµë‹ˆë‹¤. ìš”ì•½í•˜ë©´ ì´ë©”ì¼ êµ¬ì¡°ëŠ” ìœ ìš©í•œ ì •ë³´ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUN8aNTCjnzQ"
      },
      "source": [
        "ì´ì œ ì´ë©”ì¼ í—¤ë”ë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2hXEGO3jnzR"
      },
      "outputs": [],
      "source": [
        "for header, value in spam_emails[0].items():\n",
        "    print(header,\":\",value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcRYSNd6jnzR"
      },
      "source": [
        "ë³´ë‚¸ì‚¬ëŒì˜ ì´ë©”ì¼ ì£¼ì†Œì™€ ê°™ì´ í—¤ë”ì—ëŠ” ìœ ìš©í•œ ì •ë³´ê°€ ë§ì´ ìˆì§€ë§Œ ì—¬ê¸°ì„œëŠ” `Subject` í—¤ë”ë§Œ ë‹¤ë¤„ ë³´ê² ìŠµë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnXn_kn9jnzR"
      },
      "outputs": [],
      "source": [
        "spam_emails[0][\"Subject\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVPblwHTjnzR"
      },
      "source": [
        "ì¢‹ìŠµë‹ˆë‹¤. ë°ì´í„°ì—ë¥¼ ë” ì‚´í´ë³´ê¸° ì „ì— í›ˆë ¨ ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë‚˜ëˆ„ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wipl27bDjnzR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = np.array(ham_emails + spam_emails, dtype=object)\n",
        "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU2-w-txjnzR"
      },
      "source": [
        "ì´ì œ ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤. ë¨¼ì € HTMLì„ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì´ ì‘ì—…ì—ëŠ” ë‹¹ì—°íˆ [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ëŠ”ê²Œ ì¢‹ì§€ë§Œ ì˜ì¡´ì„±ì„ ì¤„ì´ê¸° ìœ„í•´ì„œ ì •ê·œì‹ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€ê°• ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤([unÌ¨hoÍly radianÍceÍ destroÒ‰ying all enliÌÍ„Ì‚Í„ghtenment](https://stackoverflow.com/a/1732454/38626)ì˜ ìœ„í—˜ì—ë„ ë¶ˆêµ¬í•˜ê³ ). ë‹¤ìŒ í•¨ìˆ˜ëŠ” `<head>` ì„¹ì…˜ì„ ì‚­ì œí•˜ê³  ëª¨ë“  `<a>` íƒœê·¸ë¥¼ HYPERLINK ë¬¸ìë¡œ ë°”ê¿‰ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ëª¨ë“  HTML íƒœê·¸ë¥¼ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë§Œ ë‚¨ê¹ë‹ˆë‹¤. ë³´ê¸° í¸í•˜ê²Œ ì—¬ëŸ¬ê°œì˜ ê°œí–‰ ë¬¸ìë¥¼ í•˜ë‚˜ë¡œ ë§Œë“¤ê³  (`&gt;`ë‚˜ `&nbsp;` ê°™ì€) html ì—”í‹°í‹°ë¥¼ ë³µì›í•©ë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ob-wiJOZjnzR"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from html import unescape\n",
        "\n",
        "def html_to_plain_text(html):\n",
        "    text = re.sub('<head.*?>.*?</head>', '', html, flags=re.M | re.S | re.I)\n",
        "    text = re.sub('<a\\s.*?>', ' HYPERLINK ', text, flags=re.M | re.S | re.I)\n",
        "    text = re.sub('<.*?>', '', text, flags=re.M | re.S)\n",
        "    text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n",
        "    return unescape(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8lN22yTjnzR"
      },
      "source": [
        "ì˜ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•´ ë³´ê² ìŠµë‹ˆë‹¤. ë‹¤ìŒì€ HTML ìŠ¤íŒ¸ì…ë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wg_jGTsCjnzR"
      },
      "outputs": [],
      "source": [
        "html_spam_emails = [email for email in X_train[y_train==1]\n",
        "                    if get_email_structure(email) == \"text/html\"]\n",
        "sample_html_spam = html_spam_emails[7]\n",
        "print(sample_html_spam.get_content().strip()[:1000], \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7KWRHd4jnzR"
      },
      "source": [
        "ë³€í™˜ëœ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZVhYBlRjnzR"
      },
      "outputs": [],
      "source": [
        "print(html_to_plain_text(sample_html_spam.get_content())[:1000], \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bBVuoVnjnzR"
      },
      "source": [
        "ì•„ì£¼ ì¢‹ìŠµë‹ˆë‹¤! ì´ì œ í¬ë§·ì— ìƒê´€ì—†ì´ ì´ë©”ì¼ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ì„œ ì¼ë°˜ í…ìŠ¤íŠ¸ë¥¼ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ê² ìŠµë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqLosjqPjnzR"
      },
      "outputs": [],
      "source": [
        "def email_to_text(email):\n",
        "    html = None\n",
        "    for part in email.walk():\n",
        "        ctype = part.get_content_type()\n",
        "        if not ctype in (\"text/plain\", \"text/html\"):\n",
        "            continue\n",
        "        try:\n",
        "            content = part.get_content()\n",
        "        except: # in case of encoding issues\n",
        "            content = str(part.get_payload())\n",
        "        if ctype == \"text/plain\":\n",
        "            return content\n",
        "        else:\n",
        "            html = content\n",
        "    if html:\n",
        "        return html_to_plain_text(html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YN6tid0jnzR"
      },
      "outputs": [],
      "source": [
        "print(email_to_text(sample_html_spam)[:100], \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzxbBI7ojnzR"
      },
      "source": [
        "ì–´ê°„ ì¶”ì¶œì„ í•´ë³´ì£ ! ì´ ì‘ì—…ì„ í•˜ë ¤ë©´ ìì—°ì–´ ì²˜ë¦¬ íˆ´í‚·([NLTK](http://www.nltk.org/))ì„ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ê°„ë‹¨íˆ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ë¨¼ì € virtualenv í™˜ê²½ì„ í™œì„±í™”ì‹œì¼œì•¼ í•©ë‹ˆë‹¤. ë³„ë„ì˜ í™˜ê²½ì´ ì—†ë‹¤ë©´ ì–´ë“œë¯¼ ê¶Œí•œì´ í•„ìš”í• ì§€ ëª¨ë¦…ë‹ˆë‹¤. ì•„ë‹ˆë©´ `--user` ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”):\n",
        "\n",
        "`$ pip install nltk`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZvxJPWhjnzR"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import nltk\n",
        "\n",
        "    stemmer = nltk.PorterStemmer()\n",
        "    for word in (\"Computations\", \"Computation\", \"Computing\", \"Computed\", \"Compute\", \"Compulsive\"):\n",
        "        print(word, \"=>\", stemmer.stem(word))\n",
        "except ImportError:\n",
        "    print(\"Error: stemming requires the NLTK module.\")\n",
        "    stemmer = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKPkR9u4jnzR"
      },
      "source": [
        "ì¸í„°ë„· ì£¼ì†ŒëŠ” \"URL\" ë¬¸ìë¡œ ë°”ê¾¸ê² ìŠµë‹ˆë‹¤. [ì •ê·œì‹](https://mathiasbynens.be/demo/url-regex)ì„ í•˜ë“œ ì½”ë”©í•  ìˆ˜ë„ ìˆì§€ë§Œ [urlextract](https://github.com/lipoja/URLExtract) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤. ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•©ë‹ˆë‹¤(ë¨¼ì € virtualenv í™˜ê²½ì„ í™œì„±í™”ì‹œì¼œì•¼ í•©ë‹ˆë‹¤. ë³„ë„ì˜ í™˜ê²½ì´ ì—†ë‹¤ë©´ ì–´ë“œë¯¼ ê¶Œí•œì´ í•„ìš”í• ì§€ ëª¨ë¦…ë‹ˆë‹¤. ì•„ë‹ˆë©´ `--user` ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”):\n",
        "\n",
        "`$ pip install urlextract`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXlqQR9OjnzR"
      },
      "outputs": [],
      "source": [
        "# ì½”ë©ì—ì„œ ì´ ë…¸íŠ¸ë¶ì„ ì‹¤í–‰í•˜ë ¤ë©´ ë¨¼ì € pip install urlextractì„ ì‹¤í–‰í•©ë‹ˆë‹¤\n",
        "try:\n",
        "    import google.colab\n",
        "    %pip install -q -U urlextract\n",
        "except ImportError:\n",
        "    pass # ì½”ë©ì—ì„œëŠ” ì‹¤í–‰ë˜ì§€ ì•ŠìŒ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRlnrt7sjnzS"
      },
      "source": [
        "**ë…¸íŠ¸:** ì£¼í”¼í„° ë…¸íŠ¸ë¶ì—ì„œëŠ” í•­ìƒ `!pip` ëŒ€ì‹  `%pip`ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. `!pip`ëŠ” ë‹¤ë¥¸ í™˜ê²½ì— ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë°˜ë©´ `%pip`ëŠ” í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ í™˜ê²½ì— ì„¤ì¹˜ë©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0S6mUIojnzS"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import urlextract # ë£¨íŠ¸ ë„ë©”ì¸ ì´ë¦„ì„ ë‹¤ìš´ë¡œë“œí•˜ê¸° ìœ„í•´ ì¸í„°ë„· ì—°ê²°ì´ í•„ìš”í• ì§€ ëª¨ë¦…ë‹ˆë‹¤\n",
        "    \n",
        "    url_extractor = urlextract.URLExtract()\n",
        "    print(url_extractor.find_urls(\"Will it detect github.com and https://youtu.be/7Pq-S557XQU?t=3m32s\"))\n",
        "except ImportError:\n",
        "    print(\"Error: replacing URLs requires the urlextract module.\")\n",
        "    url_extractor = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUPzOV0mjnzS"
      },
      "source": [
        "ì´ë“¤ì„ ëª¨ë‘ í•˜ë‚˜ì˜ ë³€í™˜ê¸°ë¡œ ì—°ê²°í•˜ì—¬ ì´ë©”ì¼ì„ ë‹¨ì–´ ì¹´ìš´íŠ¸ë¡œ ë°”ê¿€ ê²ƒì…ë‹ˆë‹¤. íŒŒì´ì¬ì˜ `split()` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ë©´ êµ¬ë‘£ì ê³¼ ë‹¨ì–´ ê²½ê³„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ì„ ë‹¨ì–´ë¡œ ë°”ê¿‰ë‹ˆë‹¤. ì´ ë°©ë²•ì´ ë§ì€ ì–¸ì–´ì— í†µí•˜ì§€ë§Œ ì „ë¶€ëŠ” ì•„ë‹™ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì¤‘êµ­ì–´ì™€ ì¼ë³¸ì–´ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë‹¨ì–´ ì‚¬ì´ì— ê³µë°±ì„ ë‘ì§€ ì•ŠìŠµë‹ˆë‹¤. ë² íŠ¸ë‚¨ì–´ëŠ” ìŒì ˆ ì‚¬ì´ì— ê³µë°±ì„ ë‘ê¸°ë„ í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ë°ì´í„°ì…‹ì´ (ê±°ì˜) ì˜ì–´ë¡œ ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ë¬¸ì œì—†ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjoX9LQgjnzS"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True,\n",
        "                 replace_urls=True, replace_numbers=True, stemming=True):\n",
        "        self.strip_headers = strip_headers\n",
        "        self.lower_case = lower_case\n",
        "        self.remove_punctuation = remove_punctuation\n",
        "        self.replace_urls = replace_urls\n",
        "        self.replace_numbers = replace_numbers\n",
        "        self.stemming = stemming\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X, y=None):\n",
        "        X_transformed = []\n",
        "        for email in X:\n",
        "            text = email_to_text(email) or \"\"\n",
        "            if self.lower_case:\n",
        "                text = text.lower()\n",
        "            if self.replace_urls and url_extractor is not None:\n",
        "                urls = list(set(url_extractor.find_urls(text)))\n",
        "                urls.sort(key=lambda url: len(url), reverse=True)\n",
        "                for url in urls:\n",
        "                    text = text.replace(url, \" URL \")\n",
        "            if self.replace_numbers:\n",
        "                text = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', 'NUMBER', text)\n",
        "            if self.remove_punctuation:\n",
        "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
        "            word_counts = Counter(text.split())\n",
        "            if self.stemming and stemmer is not None:\n",
        "                stemmed_word_counts = Counter()\n",
        "                for word, count in word_counts.items():\n",
        "                    stemmed_word = stemmer.stem(word)\n",
        "                    stemmed_word_counts[stemmed_word] += count\n",
        "                word_counts = stemmed_word_counts\n",
        "            X_transformed.append(word_counts)\n",
        "        return np.array(X_transformed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbJwg7arjnzS"
      },
      "source": [
        "ì´ ë³€í™˜ê¸°ë¥¼ ëª‡ ê°œì˜ ì´ë©”ì¼ì— ì ìš©í•´ ë³´ê² ìŠµë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKas7zZnjnzS"
      },
      "outputs": [],
      "source": [
        "X_few = X_train[:3]\n",
        "X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\n",
        "X_few_wordcounts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rVLF7EJjnzS"
      },
      "source": [
        "ì œëŒ€ë¡œ ì‘ë™í•˜ëŠ” ê²ƒ ê°™ë„¤ìš”!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SliEGj_jjnzS"
      },
      "source": [
        "ì´ì œ ë‹¨ì–´ ì¹´ìš´íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ì„œ ë˜ ë‹¤ë¥¸ ë³€í™˜ê¸°ë¥¼ ë§Œë“¤ê² ìŠµë‹ˆë‹¤. ì´ ë³€í™˜ê¸°ëŠ” (ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ ìˆœìœ¼ë¡œ ì •ë ¬ëœ) ì–´íœ˜ ëª©ë¡ì„ êµ¬ì¶•í•˜ëŠ” `fit()` ë©”ì„œë“œì™€ ì–´íœ˜ ëª©ë¡ì„ ì‚¬ìš©í•´ ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ë°”ê¾¸ëŠ” `transform()` ë©”ì„œë“œë¥¼ ê°€ì§‘ë‹ˆë‹¤. ì¶œë ¥ì€ í¬ì†Œ í–‰ë ¬ì´ ë©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lNKnQNPjnzS"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vocabulary_size=1000):\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "    def fit(self, X, y=None):\n",
        "        total_count = Counter()\n",
        "        for word_count in X:\n",
        "            for word, count in word_count.items():\n",
        "                total_count[word] += min(count, 10)\n",
        "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
        "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
        "        return self\n",
        "    def transform(self, X, y=None):\n",
        "        rows = []\n",
        "        cols = []\n",
        "        data = []\n",
        "        for row, word_count in enumerate(X):\n",
        "            for word, count in word_count.items():\n",
        "                rows.append(row)\n",
        "                cols.append(self.vocabulary_.get(word, 0))\n",
        "                data.append(count)\n",
        "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9o7Kq-5ajnzS"
      },
      "outputs": [],
      "source": [
        "vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=10)\n",
        "X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)\n",
        "X_few_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fqb4LtzjnzS"
      },
      "outputs": [],
      "source": [
        "X_few_vectors.toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzg-PJGzjnzS"
      },
      "source": [
        "ì´ í–‰ë ¬ì€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ë‚˜ìš”? ì„¸ ë²ˆì§¸ í–‰ì˜ ì²« ë²ˆì§¸ ì—´ì˜ 65ëŠ” ì„¸ ë²ˆì§¸ ì´ë©”ì¼ì´ ì–´íœ˜ ëª©ë¡ì— ì—†ëŠ” ë‹¨ì–´ë¥¼ 65ê°œ ê°€ì§€ê³  ìˆë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤. ê·¸ ë‹¤ìŒì˜ 0ì€ ì–´íœ˜ ëª©ë¡ì— ìˆëŠ” ì²« ë²ˆì§¸ ë‹¨ì–´ê°€ í•œ ë²ˆë„ ë“±ì¥í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ëœ»ì´ê³  ê·¸ ë‹¤ìŒì˜ 1ì€ í•œ ë²ˆ ë‚˜íƒ€ë‚œë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤. ì´ ë‹¨ì–´ë“¤ì´ ë¬´ì—‡ì¸ì§€ í™•ì¸í•˜ë ¤ë©´ ì–´íœ˜ ëª©ë¡ì„ ë³´ë©´ ë©ë‹ˆë‹¤. ì²« ë²ˆì§¸ ë‹¨ì–´ëŠ” \"the\"ì´ê³  ë‘ ë²ˆì§¸ ë‹¨ì–´ëŠ” \"of\"ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yf8KHSyJjnzS",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "vocab_transformer.vocabulary_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCGkx584jnzS"
      },
      "source": [
        "ì´ì œ ìŠ¤íŒ¸ ë¶„ë¥˜ê¸°ë¥¼ í›ˆë ¨ì‹œí‚¬ ì¤€ë¹„ë¥¼ ë§ˆì³¤ìŠµë‹ˆë‹¤! ì „ì²´ ë°ì´í„°ì…‹ì„ ë³€í™˜ì‹œì¼œë³´ì£ :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MS8EeowCjnzS"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "preprocess_pipeline = Pipeline([\n",
        "    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n",
        "    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n",
        "])\n",
        "\n",
        "X_train_transformed = preprocess_pipeline.fit_transform(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-oIWAKSjnzS"
      },
      "source": [
        "**Note**: to be future-proof, we set `solver=\"lbfgs\"` since this will be the default value in Scikit-Learn 0.22."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVyK_jO-jnzS"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "log_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42)\n",
        "score = cross_val_score(log_clf, X_train_transformed, y_train, cv=3, verbose=3)\n",
        "score.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IKtHEF2jnzS"
      },
      "source": [
        "98.5%ê°€ ë„˜ë„¤ìš”. ì²« ë²ˆì§¸ ì‹œë„ì¹˜ê³  ë‚˜ì˜ì§€ ì•ŠìŠµë‹ˆë‹¤! :) ê·¸ëŸ¬ë‚˜ ì´ ë°ì´í„°ì…‹ì€ ë¹„êµì  ì‰¬ìš´ ë¬¸ì œì…ë‹ˆë‹¤. ë” ì–´ë ¤ìš´ ë°ì´í„°ì…‹ì— ì ìš©í•´ ë³´ë©´ ê²°ê³¼ê°€ ê·¸ë¦¬ ë†’ì§€ ì•Šì„ ê²ƒì…ë‹ˆë‹¤. ì—¬ëŸ¬ê°œì˜ ëª¨ë¸ì„ ì‹œë„í•´ ë³´ê³  ì œì¼ ì¢‹ì€ ê²ƒì„ ê³¨ë¼ êµì°¨ ê²€ì¦ìœ¼ë¡œ ì„¸ë°€í•˜ê²Œ íŠœë‹í•´ ë³´ì„¸ìš”.\n",
        "\n",
        "í•˜ì§€ë§Œ ì „ì²´ ë‚´ìš©ì„ íŒŒì•…í–ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œ ë©ˆì¶”ê² ìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ì •ë°€ë„/ì¬í˜„ìœ¨ì„ ì¶œë ¥í•´ ë³´ê² ìŠµë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgRr6lFbjnzS"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "X_test_transformed = preprocess_pipeline.transform(X_test)\n",
        "\n",
        "log_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42)\n",
        "log_clf.fit(X_train_transformed, y_train)\n",
        "\n",
        "y_pred = log_clf.predict(X_test_transformed)\n",
        "\n",
        "print(\"ì •ë°€ë„: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
        "print(\"ì¬í˜„ìœ¨: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))"
      ]
    }
  ]
}